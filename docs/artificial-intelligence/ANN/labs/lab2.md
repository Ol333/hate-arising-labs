[Главная]({{ site.baseurl }}) >> [Искусстенный интеллект]({{ site.baseurl }}/artificial-intelligence/index.html) >> [Искусстенные нейронные сети]({{ site.baseurl }}/artificial-intelligence/index.html)

# Градиентный спуск и Правило обучения Уидроу-Хоффа. Обучение однослойной нейронной сети

Читаем cтр. 16 – 24. в учебном пособии [Гафаров Ф.М. Искусственные нейронные сети и приложения](https://kpfu.ru/staff_files/F1493580427/NejronGafGal.pdf).


## Задание

### Шаг 1
Модифицировать класс нейрона таким образом, чтобы в нем можно было задавать произвольное количество входов. Добавить инициализацию значений весов случайными малыми значениями [0.001; 0.2].

```python
class Neuron:
	def __init__(self, n)  
	#n – количество входов нейрона
	
	def predict(self, x : list) -> float
```

### Шаг 2
Создать класс однослойной нейронной сети с возможностью произвольного задания количества нейронов.

```python
class NeuralNetwork:
	def __init__(self, n)  
	#n – количество нейронов

	def predict(self, x : list) -> list  
	#от каждого нейрона по одному значению на выход

	def fit(self, x : list, y : list)
```

### Шаг 3
#### Первый вариант обучения

Нейрон с двумя входами и линейной функцией активации:
$y = 3x_1 + 2x_2$

Данные для обучения:

| № | $x_1$ | $x_2$ | y |
| - | - | - | - |
| 1 | 1 | 3 | 8 |
| 2 | 2 | 4 | 11 |
| 3 | 1 | 5 | 9 |
| n | .. | .. | .. |


Расчет ошибки предсказания для первого примера:

$y_{1pred} = 3 * 1 + 2 * 3 = 9$

$ε = 1$

$(3 + Δ_1) * 1 + (2 + Δ_1) * 3 = 8$

$4Δ_1 = 8 - 9$

$Δ_1 = -0.25$

Нейрон с обновленными весами:
$y = 2.75x_1 + 1.75x_2$	


Формула для обновления весов нейронов (для `NeuralNetwork.fit_1()`):

$ ω_{ij}^{t+1}=ω_{ij}^t + \frac {y_i - \sum_{j=1}^n ω_{ij}^t x_{ij}}{\sum_{j=1}^n ω_{ij}^t }$ <a id="eq_1">(1)</a>

### Шаг 4
#### Второй вариант обучения
Аналитическое решение (математические формулы) – это идеально, если оно есть. В случаях, когда красивой формулы нет или она есть, но ее надо представить в виде программного кода, мы обращаемся к вычислительной математике. Надо понимать, что машинные числа являются дискретной проекцией вещественных чисел на конкретную архитектуру компьютера, следовательно, неизбежно падает точность.
Оптимизация подразумевает нахождение экстремума целевой функции, когда заданы переменные (что можно изменять) и ограничения (какие условия должны обязательно выполняться).
Большинство численных методов оптимизации – итеративные: в цикле определяется новое значение X (предположительно, приближающееся к глобальному экстремуму), для которого рассчитывается значение Y. Критерием останова для них является момент, когда изменение значения функции за шаг становится меньше заданной точности ε.

##### Методы оптимизации. Градиентный спуск.
Как нам известно, градиент – вектор, указывающий направление наискорейшего роста некоторой скалярной величины. Поэтому, чтобы найти минимум функции, нам надо двигаться в сторону антиградиента (это все при условии, что нам известна формула, выражающая функцию, и она дифференцируема).

![градиентный спуск](http://www.machinelearning.ru/wiki/images/f/f6/Grad1.PNG)

$\bar x ^{k+1} = \bar x^k - α∇Φ(\bar x^k )$,	<a id="eq_2">(2)</a>

где $∇Φ(\bar x) = \left(\begin{matrix}
\frac {∂Φ}{∂x_1} \\ \frac {∂Φ}{∂x_2} \\ .. \\ ..
\end{matrix} \right) $  ,

$α$ – задаваемый шаг, лучше делать меньше 0.01.


В случае обучения нейронной сети, оптимизируемая функция – функция ошибки обучения. Пусть будет среднеквадратичная ошибка:

$E=\frac 12 (y-y_{расчетный} )^2$	<a id="eq_3">(3)</a>

Подставив в формулу [(2)](#eq_2) вес c индексом (вместо вектора) и оптимизируемую функцию, получим:

$ω_{ij}^{t+1} = ω_{ij}^t - α \frac {∂(\frac 12 ( \sum_{j=1}^n ω_{ij}^t x_{ij}-y_i)^2}{∂x_i}$

Получаем формулу для обновления весов нейронов (для `NeuralNetwork.fit_2()`):

$ω_{ij}^{t+1} = ω_{ij}^t - α \left( \sum_{j=1}^n (ω_{ij}^t x_{ij}) - y_i \right) * x_{ij} $	<a id="eq_4">(4)</a>

### Шаг 5
* В теле программы создаем 2 одинаковых сети из одного нейрона с двумя входами. Одна из сетей будет обучаться по формуле [(1)](#eq_1), вторая – по [(4)](#eq_4).
* Берем файл с данными, подготовленный во второй лабораторной по другой дисциплине.
* Самостоятельно делим выборку на обучающую (80%) и тестовую (20%). НЕ используя готовые библиотеки.
* Обучаем 2 нейронные сети на этих данных.
* Считаем среднеквадратичную ошибку [(3)](#eq_3) на тестовой выборке.
* Смотрим, какие веса получились.

### Шаг 6
В теле программы создаем сеть из 3-х нейронов. У каждого нейрона 6 входов.
Обучаем нейронную сеть на данных из файла [2lab_data.csv](https://disk.yandex.ru/d/2TX_9LT_uHmXZg).

### Шаг 7
Обучаем сеть из 1 нейрона с 2 входами на распознавание наличия цвета на картинке (берем файл с данными, подготовленный в конце второй лабораторной по другой дисциплине). Если не обучается, попробуйте подобрать веса вручную.

[← Лабораторная работа №1]({{ site.baseurl }}/artificial-intelligence/ANN/labs/lab1.html)