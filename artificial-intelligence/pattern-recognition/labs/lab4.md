<ol class="breadcrumb">
  <li class="breadcrumb-item"><a href="{{ site.baseurl }}">Главная</a></li>
  <li class="breadcrumb-item"><a href="{{ site.baseurl }}/artificial-intelligence/index.html">ИИ</a></li>
  <li class="breadcrumb-item"><a href="{{ site.baseurl }}/artificial-intelligence/pattern-recognition/index.html">Распознавание образов</a></li>
  <li class="breadcrumb-item active">ЛР №4</li>
</ol>

<nav>
  <ul></ul>
</nav>

# Сегментация

*Также может быть известно как маскирование.*

Помимо обнаружения объектов на изображении, обычно в границах прямоугольных рамок, и их классификации, можно **сегментировать** области изображения по классу, т. е. создать маску, которая будет разделять изображение на несколько классов. Каждому пикселю изображения присваивается определённый класс, позволяющий выделить интересующие области (фон) и/или объекты. Посмотреть: [Сегментация изображений (ИТМО)](https://neerc.ifmo.ru/wiki/index.php?title=%D0%A1%D0%B5%D0%B3%D0%BC%D0%B5%D0%BD%D1%82%D0%B0%D1%86%D0%B8%D1%8F_%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B9){:target="_blank"}, [презентация о сегментации](https://logic.pdmi.ras.ru/~sergey/teaching/udata18/20-segmentation.pdf){:target="_blank"}.

**Cемантическая сегментация (semantic segmentation)** каждый пиксель изображения классифицируется в один из заданных классов. При этом все объекты одного класса помечаются одинаково, без различий между отдельными экземплярами. *Например, если на изображении есть три автомобиля, семантическая сегментация выделит все пиксели, принадлежащие автомобилям, и присвоит им один и тот же класс ("автомобиль"), не разделяя автомобили на отдельные экземпляры.*

**Сегментация экземпляров (instance segmentation)** не только определяет класс каждого пикселя, но и разделяет объекты одного класса на отдельные экземпляры. То есть, каждый объект того же класса выделяется своей уникальной маской. *Возвращаясь к примеру с автомобилями, каждый автомобиль будет выделен индивидуально, с отдельными масками, чтобы различать эти экземпляры.* Необходимо для случаев, когда важно идентифицировать и различать отдельные объекты, например, в системах автономного вождения или в приложениях компьютерного зрения для подсчёта и анализа объектов.


## Fully Convolutional Network (FCN)

Fully Convolutional Network (FCN) стала одной из первых архитектур, специально разработанных для задач семантической сегментации изображений. В отличие от традиционных сверточных нейронных сетей, которые завершаются полносвязными слоями для классификации, FCN полностью заменяет эти слои на свёрточные, что позволяет ей обрабатывать изображения произвольного размера и предсказывать класс для каждого пикселя.

[Long, J., Shelhamer, E., & Darrell, T. (2015). "Fully Convolutional Networks for Semantic Segmentation."](https://arxiv.org/abs/1411.4038){:target="_blank"}

<div class="card border-primary mb-2" style="max-width: 40rem;">
  <div class="card-body">
    <img src="{{ site.baseurl }}/img/FCN_2.svg"
        alt="FCN" focusable="false" width="100%"
        class="d-block user-select-none" />
  </div>
</div>

[*](https://habr.com/ru/companies/oleg-bunin/articles/340184/){:target="_blank"} FCN может пробежаться обученным сверточным окном по всему изображению и построить как бы тепловую карту на выходе — где в этом конкретном изображении находится конкретный класс. Вы можете построить, например, 1000 этих Heatmap для всех своих классов и потом использовать это для определения места нахождения объекта на картинке.

FCN может быть построена на основе известных архитектур, таких как VGG или ResNet, для извлечения признаков. В процессе работы изображения проходят через несколько свёрточных слоёв и операций субдискретизации (пуллинга), что снижает разрешение карты признаков.

Чтобы получить карту сегментации с разрешением, аналогичным исходному изображению, FCN использует техники апсемплинга (например, транспонированную свёртку↓). Эти слои помогают восстанавливать пространственное разрешение до исходного размера изображения.

Для улучшения точности сегментации FCN вводит скип-соединения (skip connections), которые позволяют объединять признаки с разных уровней абстракции. Это помогает учитывать как глобальные контексты (например, формы объектов), так и локальные детали (например, границы).

### Транспонированная свёртка

***Обратная свертка (deconvolution)***. Но, поскольку в цифровой обработке сигналов это слово занято совсем другой вещью — похожей, но не такой, то лучше использовать другой термин - **транспонированная свертка (transposed convolution)**.

[*](https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11){:target="_blank"} Транспонированный сверточный слой обычно используется для повышения частоты дискретизации, т. е. для генерации выходной карты признаков, которая имеет пространственное измерение больше, чем у входной карты признаков. Как и сверточный слой, также определяется отступом и шагом.

Реализацию транспонированного сверточного слоя можно лучше объяснить как четырехэтапный процесс:
1. Рассчитать новые параметры $z$ и $p'$.
2. Между каждой строкой и столбцами ввода вставить $z$ нулей. Это увеличит размер ввода до $(2 * i - 1) \times (2 * i - 1)$.
3. Дополнить измененное входное изображение нулями в количестве $p'$.
4. Выполнить стандартную свертку изображения, полученного на шаге 3, с шагом 1.

<div class="card border-primary mb-2" style="max-width: 60rem;">
  <div class="card-body">
    <img src="{{ site.baseurl }}/img/transposed_convolution.png"
        alt="Транспонированная свёртка" focusable="false" width="100%"
        class="d-block user-select-none" />
  </div>
</div>

- ($s,p$) – параметры

- $z$ – число нулей, вставляемых между столбцами и строками)

- $p’$ – число нулей вокруг исходного массива

- $s’$ – шаг сдвига ядра

$o=(i-1) \times s + k - 2p$ - Размер выходного массива ($i$ - размер входного массива)

## U-Net

U-Net — это архитектура нейронной сети, [разработанная](https://arxiv.org/abs/1505.04597){:target="_blank"} специально для задач семантической сегментации изображений. Она получила широкое распространение в медицинских приложениях и других областях, требующих высокоточного выделения объектов. Почитать [здесь](https://neurohive.io/ru/vidy-nejrosetej/u-net-image-segmentation/){:target="_blank"}.

<div class="card border-primary mb-2" style="max-width: 40rem;">
  <div class="card-body">
    <img src="{{ site.baseurl }}/img/u-net-illustration-correct-scale2.svg"
        alt="U-Net" focusable="false" width="100%"
        class="d-block user-select-none" />
  </div>
</div>

Основная структура U-Net напоминает букву "U" и состоит из двух главных частей:

* Энкодер (Сжимающий путь): Эта часть сети уменьшает размер изображения, извлекая важные признаки. Она состоит из серии свёрточных и пуллинговых слоёв, которые постепенно уменьшают пространственные размеры изображения, увеличивая при этом количество каналов. Таким образом, энкодер учится различать высокоуровневые признаки, такие как формы и контуры объектов.
* Декодер (Восстанавливающий путь): В этой части размер изображения восстанавливается до исходного, используя слои транспонированной свёртки (up-convolution) и другие техники. При этом декодер комбинирует извлечённые признаки с их высокоуровневыми версиями, чтобы восстановить мелкие детали и границы объектов.
* Скипы (Пропуски): Одной из ключевых особенностей U-Net является использование "skip connections" между слоями энкодера и декодера. Они позволяют передавать информацию от слоёв энкодера с высоким разрешением в соответствующие слои декодера, что помогает лучше восстанавливать детали изображения.


## Mask R-CNN

Mask R-CNN — это архитектура нейронной сети, разработанная для решения задачи instance segmentation, построена на базе object detection модели Faster R-CNN. С помощью R-CNN находятся гипотезы, а затем в их границах происходит сегментация объектов.
[He, K., Gkioxari, G., Dollár, P., & Girshick, R. (2017). "Mask R-CNN."](https://arxiv.org/abs/1703.06870){:target="_blank"}

<div class="card border-primary mb-2" style="max-width: 45rem;">
  <div class="card-body">
    <img src="{{ site.baseurl }}/img/mask_r_cnn.webp"
        alt="Mask R-CNN" focusable="false" width="100%"
        class="d-block user-select-none" />
  </div>
</div>

Основные части Mask R-CNN:
1. Backbone Network: В качестве основы используется модель для извлечения признаков из изображения, например, ResNet или ResNeXt с дополнительной структурой Feature Pyramid Network (FPN). Это помогает извлекать признаки разного уровня, что особенно важно для объектов различных размеров.
2. Region Proposal Network (RPN): Этот компонент, заимствованный из Faster R-CNN, предлагает потенциальные регионы, которые могут содержать объекты. Эти регионы проходят через классификатор и регрессор, чтобы предсказать классы объектов и уточнить их местоположение.
3. Классификация и Локализация: Как и в Faster R-CNN, предложенные регионы классифицируются, и модель уточняет координаты их ограничивающих рамок. Но Mask R-CNN добавляет третий, параллельный выход.
4. Mask Head: Этот новый компонент — масочный выход — предназначен для предсказания пиксельных масок для каждого предложенного объекта. Для каждой маски используется небольшой сверточный слой, который выдаёт бинарную маску, соответствующую форме объекта. Важно, что процесс сегментации выполняется отдельно от классификации, что повышает точность выделения объектов.


## DeepLab

DeepLab — это серия моделей, разработанных для улучшенной семантической сегментации изображений, с акцентом на высокое разрешение и точность. Основная идея заключается в использовании специальных техник, позволяющих захватывать как локальные, так и глобальные контексты изображения, что важно для распознавания объектов с различной детализацией и плотностью. [Chen, L.-C. et al. (2017). "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs."](https://arxiv.org/abs/1606.00915){:target="_blank"}; [Chen, L.-C. et al. (2018). "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation."](https://arxiv.org/abs/1802.02611){:target="_blank"}

<div class="card border-primary mb-2" style="max-width: 40rem;">
  <div class="card-body">
    <img src="{{ site.baseurl }}/img/DeepLabV3+.jpeg"
        alt="DeepLabV3+" focusable="false" width="100%"
        class="d-block user-select-none" />
  </div>
</div>

Модель DeepLab прошла несколько этапов развития, начиная с DeepLabV1 и заканчивая более мощными версиями DeepLabV3 и DeepLabV3+. Основные особенности архитектуры:

1. Atrous Convolutions: Одной из ключевых идей DeepLab является использование атрезионных (или дырчатых) свёрток. Эти свёртки позволяют увеличить поле зрения фильтра без увеличения количества параметров или снижения разрешения выходной карты признаков. В результате модель может улавливать более глобальные контексты (для сегментации объектов большого размера).
2. Atrous Spatial Pyramid Pooling (ASPP): ASPP модуль объединяет несколько слоёв с атрезионными свёртками, настроенными на разные коэффициенты разрежения. Это помогает модели одновременно захватывать признаки с разных масштабов, улучшая способность различать как мелкие, так и крупные объекты.
3. Использование сетей (например, ResNet или Xception) в качестве базовой архитектуры для извлечения признаков. В DeepLabV3+ был добавлен более сложный декодирующий блок, который улучшает восстановление пространственных деталей.
4. Чтобы устранить ошибки вдоль границ объектов, DeepLab включает в себя механизм пост-обработки Conditional Random Fields (CRFs), который может уточнять маски сегментации, используя информацию о контексте и текстурах.


## Segment Anything Model (SAM)

Segment Anything Model (SAM) — это универсальная архитектура для сегментации изображений, представленная компанией Meta*. SAM основана на идее создания универсальной модели, которая способна сегментировать практически любой объект на изображении с минимальным или даже нулевым количеством примеров для обучения. [Kirillov, A., Mintun, E., Ravi, N., et al. (2023). "Segment Anything."](https://arxiv.org/abs/2304.02643){:target="_blank"}. [Официальная документация и примеры на GitHub](https://github.com/facebookresearch/segment-anything){:target="_blank"}.

*компания Meta Platforms Inc., владеющая Facebook и Instagram, внесена в реестр экстремистских организаций, ее деятельность в России по поддержанию указанных соцсетей признана экстремистской деятельностью

<div class="card border-primary mb-2" style="max-width: 57rem;">
  <div class="card-body">
    <img src="{{ site.baseurl }}/img/SAM.svg"
        alt="SAM" focusable="false" width="100%"
        class="d-block user-select-none" />
  </div>
</div>

Ообенности SAM:

1. SAM способна выполнять как семантическую сегментацию, так и сегментацию экземпляров. Она поддерживает несколько режимов работы, включая автоматическую сегментацию объектов, сегментацию по подсказке (например, выделение объекта по щелчку пользователя) и многорежимную сегментацию.
2. Модель использует комбинированный подход, сочетающий преимущества классических свёрточных сетей (CNN) для извлечения низкоуровневых признаков и трансформеров для захвата долгосрочных зависимостей и глобального контекста.
3. SAM предоставляет возможности для интерактивного взаимодействия с пользователем. Например, пользователь может указать объект на изображении, выделив его с помощью точки, прямоугольника или контура, а модель быстро уточняет сегментацию на основе этих подсказок.
4. Благодаря предобучению на обширных наборах данных, SAM демонстрирует способность хорошо работать на изображениях, которые она ранее не видела.


## PSPNet

PSPNet (Pyramid Scene Parsing Network) — это архитектура, разработанная для более точной семантической сегментации, особенно в сложных сценах с множеством объектов и разнообразным фоном. PSPNet эффективно использует глобальный контекст изображения, чтобы улучшить сегментацию. [Zhao, H., Shi, J., Qi, X., Wang, X., & Jia, J. (2017). "Pyramid Scene Parsing Network."](https://arxiv.org/abs/1612.01105){:target="_blank"}

<div class="card border-primary mb-2" style="max-width: 57rem;">
  <div class="card-body">
    <img src="{{ site.baseurl }}/img/pspnet.svg"
        alt="PSPNet" focusable="false" width="100%"
        class="d-block user-select-none" />
  </div>
</div>

Основные особенности PSPNet:
1. Pyramid Pooling Module: Ключевым элементом этой архитектуры является Pyramid Pooling Module, который захватывает контекстные признаки на нескольких уровнях. В этом модуле изображение делится на несколько подрегионов с разными масштабами (например, 1x1, 2x2, 3x3 и 6x6). Для каждого уровня вычисляется пуллинг, и полученные признаки объединяются, чтобы захватить как локальную, так и глобальную информацию.
2. PSPNet акцентирует внимание на захвате глобального контекста, что позволяет модели различать объекты, которые могут быть схожи на уровне локальных признаков, но различаться на уровне сцены. Например, деревья и здания могут иметь разные контексты в сцене, и модель учится учитывать эти различия.
3. Backbone Network: Для извлечения признаков PSPNet использует мощные базовые сети, такие как ResNet, что позволяет эффективно обрабатывать изображения и сохранять высокое разрешение сегментационных карт.


## Задание 🌄

* Запустить модель по варианту.
* Сементировать изображения (использовать набор изображений с предыдущих лабораторных).

<div class="table-responsive">
<table class="table table-hover border-primary table-bordered ">
  <thead>
    <tr class="table-dark">
      <th scope="col">№ варианта</th>
      <th scope="col">Модель</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">1</th>
      <td>DeepLab</td>
    </tr>
    <tr>
      <th scope="row">2</th>
      <td>U-Net</td>
    </tr>
    <tr>
      <th scope="row">3</th>
      <td>FCN</td>
    </tr>
    <tr>
      <th scope="row">4</th>
      <td>SAM</td>
    </tr>
    <tr>
      <th scope="row">5</th>
      <td>PSPNet</td>
    </tr>
    <tr>
      <th scope="row">6</th>
      <td>Mask R-CNN</td>
    </tr>
   </tbody>
</table>
</div>

<div class="row">
  <div class="col-lg-12">
    <ul class="list-unstyled">
      <li class="float-end">
        <button type="button" class="btn btn-outline-primary" onclick="window.location.href='#сегментация';">Вверх</button>
      </li>
      <li  class="float-end">
       <button type="button" class="btn btn-primary" onclick="window.location.href='{{ site.baseurl }}/artificial-intelligence/pattern-recognition/labs/lab5.html';">(доп.) ЛР №5 →</button>
     </li>
      <li>
        <button type="button" class="btn btn-primary" onclick="window.location.href='{{ site.baseurl }}/artificial-intelligence/pattern-recognition/labs/lab3.html';">← ЛР №3</button>
      </li>
    </ul>
  </div>
</div>