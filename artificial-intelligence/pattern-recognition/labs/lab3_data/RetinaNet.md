# RetinaNet

## Кто предложил

T. -Y. Lin, P. Goyal, R. Girshick, K. He and P. Dollár, "Focal Loss for Dense Object Detection," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 2, pp. 318-327, 1 Feb. 2020, doi: 10.1109/TPAMI.2018.2858826. URL: [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002){:target="_blank"}.

## Описание модели

На сегодняшний день детекторы объектов с наивысшей точностью основаны на двухэтапном подходе, популяризированном R-CNN, где классификатор применяется к разреженному набору потенциальных местоположений объектов. Напротив, одноэтапные детекторы, которые применяются к регулярной плотной выборке возможных местоположений объектов, имеют потенциал быть быстрее и проще, но пока отстают от точности двухэтапных детекторов. В этой статье мы исследуем, почему это так. Мы обнаруживаем, что экстремальный дисбаланс классов переднего плана и фона, возникающий во время обучения плотных детекторов, является центральной причиной. Мы предлагаем устранить этот дисбаланс классов, изменив стандартную потерю перекрестной энтропии таким образом, чтобы она снижала вес потери, назначенной хорошо классифицированным примерам. Наша новая фокальная потеря фокусирует обучение на разреженном наборе сложных примеров и не позволяет огромному количеству легких отрицательных примеров перегружать детектор во время обучения. Чтобы оценить эффективность нашей потери, мы проектируем и обучаем простой плотный детектор, который мы называем RetinaNet. Наши результаты показывают, что при обучении с учетом фокальных потерь сеть RetinaNet способна достичь скорости предыдущих одноступенчатых детекторов, превосходя при этом по точности все существующие современные двухступенчатые детекторы.

[статья с хабра](https://habr.com/ru/articles/510560/)


## Фреймворк размещенный авторами на GitHub 

[исходники](?)

[research platform ](https://github.com/facebookresearch/Detectron), в которой в том числе реализована RetinaNet

## Пример работы

...